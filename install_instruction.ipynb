{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Python & NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use python through out this course, the purpose for this notebook is to provide you some introductions to the softwares we are going to use as well as help set up the environment in your laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment will be set up in the following procedures:\n",
    "1. Install anaconda\n",
    "2. Create a virtual environment with dependency packages\n",
    "3. Install Jupyter Notebook\n",
    "4. Run a demo in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python with Anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What & Why Anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anaconda is \"the worldâ€™s most popular and trusted data science ecosystem\". You'll find some details about it with the link I provide [here](https://www.continuum.io/what-is-anaconda). But for me, the take-away is:\n",
    "1. It helps you set up a environment for python (otherwise sometimes it is a littble bit painful if you're using windows)\n",
    "2. The virtual environment feature gives you the flexibility to switch between python versions of packages versions. You might encouter a place that you have to use python 2.7 for one project and python 3.5 for another, or version 1.0 (I made up this number) of NLTK for one project yet 1.5 for another project. Avoding installing them globally allows you to set up python environments for each project and not conficts with each other\n",
    "3. Unlike matlab, python is an open-source community. This results in an abundant and fast-developing third-party packages, which also means a lot of time you have to install those packages by yourself. Using conda can help you easily maintain those packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Go to [download page of Anaconda](https://www.continuum.io/downloads)__\n",
    "    - Download the Python 3.6 version\n",
    "    - It will be a graphical installer for both windows and mac, so you shall be fine for this step. But just in case, here are the instructions for [windows](https://docs.continuum.io/anaconda/install/windows) and [mac](https://docs.continuum.io/anaconda/install/mac-os#macos-graphical-install)\n",
    "    - There is only a command line installer in linux platfor, just follow [this](https://docs.continuum.io/anaconda/install/linux), also shouldn't be too hard\n",
    "2. __Check your installation__\n",
    "    - For windows users, please use anaconda prompt(which has already been installed), for mac and linux users, just use terminal\n",
    "    - Enter 'conda --version', you'll see something like 'conda 4.4.0'\n",
    "3. __Why Python 3?__\n",
    "    Python 3 is the latest version and lots of new advanced machine learning & deep learning tools are built upon Python 3. It is more friendly to scientific computing. But still, when conda, you can easily switch between Python 2 and Python 3\n",
    "4. __If you already installed one__\n",
    "    Check [this doc](https://docs.continuum.io/anaconda/install/update-version). In short, go to terminal and do:\n",
    "    ```\n",
    "    conda update conda\n",
    "    conda update anaconda\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Virtual Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to setup the virtual environment using conda. Normally you can do it by using instructions in this [cheat sheet](https://conda.io/docs/_downloads/conda-cheatsheet.pdf). For your convience, I wrote a yaml file and you can just use it to create the environment. All you need to do is the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In terminal (anaconda prompt for windows users), cd to the git repo you just cloned (e.g. path\\to\\ece590-08-2017\\)\n",
    "2. Enter command\n",
    "    ```\n",
    "    conda env create\n",
    "    ```\n",
    "    This will automatically load the dependencies I specified in environment.yaml and install them for you\n",
    "3. After installation is complete, for windows users, do:\n",
    "    ```\n",
    "    activate nlp-env\n",
    "    ```\n",
    "    and for mac/linux users, do:\n",
    "    ```\n",
    "    source activate nlp-env\n",
    "    ```\n",
    "    You should see the new line now start with '(nlp-env)'\n",
    "4. The last but also very important step, you need to install nb_conda to bind kernel of your virtual environment to jupyter notebook, the detailed explanations can be found in [this SO thread](https://stackoverflow.com/questions/37433363/link-conda-environment-with-jupyter-notebook). Just enter this command:\n",
    "    ```\n",
    "    conda install nb_conda\n",
    "    ```\n",
    "5. If step 4 does not work, try follow these steps:\n",
    "    1. deactivate nlp-env\n",
    "    2. run ```\n",
    "    conda install nb_conda\n",
    "    ```\n",
    "    3. activate nlp-env\n",
    "    4. run ```\n",
    "    python -m ipykernel install --user --name nlp-env\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Brief Intro of Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook is a tool I highly recommend. It is designed for data scientists to create and share codes. And now it goes far beyond that. Because it supports Markdown and LaTex, you can do a lot of things with it. Personally, I now use Jupyter Notebook for coding, light-duty debug and even taking note. To see what Jupyter Notebook is capable of, check this [cool list](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start Jupyter Notebook, just type\n",
    "```\n",
    "Jupyter Notebook\n",
    "```\n",
    "in your termial (with nlp-env activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you shall see a prompt webpage. That's the interface of Jupyter Notebook.  \n",
    "You've also probably already noticed that this page itself is actually a .ipynb file. So open the 'install_instruction.ipynb' file in the folder. Before we move to the next step, make sure the top right of the page (right side of the toolbar) says 'conda env: nlp-env'. Which means you're using the nlp-env with all the dependency libraries we just installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it says something different, like 'Python 3'. Just click 'Kernel' at toolbar, then 'Change Kernel' and select the right one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run the First NLTK Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we come to the last step. In this step, we'll start using Jupyter Notebook and try to run a simple NLTK demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, let me first introduce NLTK to you. Natural Language Toolkit is a popular (probabaly the most popular) NLP toolkit in python. It can provides you tools for downloading and managing useful corpora as well as processing text like tokenization, tagging and stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More detailed documentation can be found in their [website](http://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a 'hello world' example using NLTK (I modified from [this site](https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/)). Just use 'shift+enter' at each cell to excecute them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Bohao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like I mentioned above, NLTK helps you download and mange corpora easily. \n",
    "```\n",
    "nltk.download('popular')\n",
    "```\n",
    "just downloads most popular corpora for your (all the corpora takes more than 1GB space). Some of them are necessary for us to processing text. You can find useful info in this [SO thread](https://stackoverflow.com/questions/22211525/how-do-i-download-nltk-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi, my name is Bohao.', 'Welcome to ECE590.08: natural language processing class.', \"Don't be shy to ask any questions.\"]\n",
      "['Hi', ',', 'my', 'name', 'is', 'Bohao', '.', 'Welcome', 'to', 'ECE590.08', ':', 'natural', 'language', 'processing', 'class', '.', 'Do', \"n't\", 'be', 'shy', 'to', 'ask', 'any', 'questions', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "txt = \"Hi, my name is Bohao. Welcome to ECE590.08: natural language processing class. Don't be shy to ask any questions.\"\n",
    "\n",
    "print(sent_tokenize(txt))\n",
    "print(word_tokenize(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see in the first line of output, input text has been divided into three sentences. NLTK use a pretty robust method that it is not simply splitting by '.': ECE590.08 is still in the same sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the second line devide 'don't' into two parts, indicates this is a negation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll know how to do these 'tricks' after taking this course : )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Some Useful Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Search engine: whenever you don't know anything, like not sure about an algorhithm, don't know what to do with an error message. Just search it. You'll find __you are not the only one__. Lot's of people might already asked same question like you in StackOverflow/Quora/... You can see I cited lots of SO links in this jupyter notebook, and each one represents a search query I've made when making this tutorial\n",
    "2. Some open classes online:\n",
    "    - [Standford CS224n](http://web.stanford.edu/class/cs224n/): a very famous course, little bit deep learning oriented\n",
    "    - [Cousera NLP Course](https://www.coursera.org/learn/natural-language-processing): very good one, but no upcoming session available\n",
    "    - Also, check this [Quora page](https://www.quora.com/What-is-the-best-natural-language-processing-MOOC)\n",
    "3. Some materials:\n",
    "    - [The Hitchhikerâ€™s Guide to Python!](The Hitchhikerâ€™s Guide to Python!): this is very detailed, you'll probably find everything you need for python\n",
    "    - [Duke STA663](http://people.duke.edu/~ccc14/sta-663-2017/index.html): this is the class notes we had when taking STA663(statistical computing) last semester, the first few chapters are actually excellent introduction for you to get familiar with python and jupyter notebook and other necessary tools for data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
